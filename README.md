# LangGraph RAG Workflow API

This project demonstrates a simple LangGraph workflow integrated with a local LLM API to simulate a multi-step conversational flow.

## ðŸ“¦ Requirements

- Python 3.8+
- FastAPI
- langgraph
- httpx
- uvicorn

## ðŸš€ Running the Server

Start the FastAPI server with:

```bash
uvicorn main:app --host 0.0.0.0 --port 8001 --reload
```

## ðŸ§ª Testing Concurrent Requests

You can test the /invoke endpoint with two simultaneous queries using the following command:

```bash
curl -X POST http://localhost:8001/invoke -H "Content-Type: application/json" -d '{"query":"say something"}' \
& curl -X POST http://localhost:8001/invoke -H "Content-Type: application/json" -d '{"query":"say 3"}'
```

Each call sends a JSON payload with a query key and expects a response generated by the LangGraph workflow.