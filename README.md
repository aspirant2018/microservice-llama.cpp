# LangGraph RAG Workflow API

This project demonstrates a simple LangGraph workflow integrated with a local LLM API to simulate a multi-step conversational flow.

## Pre-requies

Make sure to run the LLaMA server before starting the FastAPI app:

```bash
llama-server -hf ggml-org/SmolVLM-500M-Instruct-GGUF -c 8080 -np 2
```
-np 2 â†’ Number of Processing Threads

## ðŸ“¦ Requirements

- Python 3.8+
- FastAPI
- langgraph
- httpx
- uvicorn

## ðŸš€ Running the Server

Start the FastAPI server with:

```bash
uvicorn main:app --host 0.0.0.0 --port 8001 --reload
```

## ðŸ§ª Testing Concurrent Requests

You can test the /invoke endpoint with two simultaneous queries using the following command:

```bash
curl -X POST http://localhost:8001/invoke -H "Content-Type: application/json" -d '{"query":"say something"}' \
& curl -X POST http://localhost:8001/invoke -H "Content-Type: application/json" -d '{"query":"say 3"}'
```

Each call sends a JSON payload with a query key and expects a response generated by the LangGraph workflow.